# 深度学习逻辑线

## 深度神经网路（DNN）

- 神经网络模型
  - input layyer-> hidden layer -> output layer 
  - 只有一层隐藏层，不同层之间通过全连接方式连接。
  - 最后的output layer后接激活函数去线性，增强模型表达能力，常用激活函数由sigmoid、tanx、softmax、ReLU等。

- 深度神经网络

  - 具有多个隐藏层的神经网络，不同隐藏层以及输入输出层之间都用全连接方式连接。

  - 每条连接线上具有权重，深度学习需要学习的参数也是连接线上的权重。

  - 每一层上的任意一个结点的输出，都是上一层所有结点的输出的对应加权和，经过激活函数之后的输出值。

  - 训练方法：反向传播算法和梯度下降算法。一开始用随机值初始化权值矩阵W和偏置项b，将DNN的输出值与真实值进行比较，如果相差过大则修改当前连接层权重，如果相差不大则修改更低一层权重，直到修改到第一层的权重。

  - 遇到的问题：梯度消失或梯度爆炸，收敛慢，过拟合：使用dropout。

    - 注：dropout正则化：每次随机选取一些神经元不参加训练，预测时这些神经元才生效，这些神经元的输出结果要乘以一个概率值。通过这种方式，降低了神经元之间的依赖性。

  - 模型图示：

    ![image](https://github.com/SongYang6571/hello-world/tree/master/images/1.PNG)

## 循环神经网络（RNN）

- RNN概述

  - 顾名思义，是带有循环的神经网络，它可以实现将上一时刻的模型参数传入到下一时刻，从而实现模型下一时刻的输入包含上一时刻的信息。它可以理解为对模型重复赋值。

  - 模型图示：

    ![image](https://github.com/SongYang6571/hello-world/tree/master/images/2.PNG)

- **LSTM网络**

  - 长时依赖问题：当我们用传统RNN模型获取之前的输入信息时，当之前的有用信息与当前预测位置相距过远时，RNN实际时无法获取前文信息的，这个问题被称为长期依赖问题，为了解决该问题，我们使用LSTM网络。

  - LSTM与传统RNN的区别：传统RNN的神经元内部重复模块结构简单，例如只有一个tanh层：

    ![image](https://github.com/SongYang6571/hello-world/tree/master/images/3.PNG)

    而LSTM内部则是更复杂的结构，不是单一的神经网络层，而是四个神经网络层的特殊交互：

    ![image](https://github.com/SongYang6571/hello-world/tree/master/images/4.PNG)

  - LSTM的核心思想：LSTM的关键是细胞状态$Ci$，LSTM通过设计的“门”对细胞状态上的信息进行添加或删除。

  - 逐步了解LSTM

    - 遗忘门：决定从细胞状态中丢弃什么信息，该门会读取$h_{t-1}$和$x_t$输出一个0-1之间的数给所有在$c_{t-1}$中的数字。

      ![image](https://github.com/SongYang6571/hello-world/tree/master/images/5.PNG)

    - 输入门：决定更新什么值，产生状态$i_t$,一个tanh层创建新的候选值向量$\tilde{C}_t$

      ![image](https://github.com/SongYang6571/hello-world/tree/master/images/6.PNG)

    - 更新细胞状态：

      ![image](https://github.com/SongYang6571/hello-world/tree/master/images/7.PNG)

    - 输出：这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。

      ![image](https://github.com/SongYang6571/hello-world/tree/master/images/8.PNG)

- 参考：<http://colah.github.io/posts/2015-08-Understanding-LSTMs/>

