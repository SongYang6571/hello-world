# 深度学习逻辑线

## 深度神经网路（DNN）

- 神经网络模型
  - input layyer-> hidden layer -> output layer 
  - 只有一层隐藏层，不同层之间通过全连接方式连接。
  - 最后的output layer后接激活函数去线性，增强模型表达能力，常用激活函数由sigmoid、tanx、softmax、ReLU等。

- 深度神经网络

  - 具有多个隐藏层的神经网络，不同隐藏层以及输入输出层之间都用全连接方式连接。

  - 每条连接线上具有权重，深度学习需要学习的参数也是连接线上的权重。

  - 每一层上的任意一个结点的输出，都是上一层所有结点的输出的对应加权和，经过激活函数之后的输出值。

  - 训练方法：反向传播算法和梯度下降算法。一开始用随机值初始化权值矩阵W和偏置项b，将DNN的输出值与真实值进行比较，如果相差过大则修改当前连接层权重，如果相差不大则修改更低一层权重，直到修改到第一层的权重。

  - 遇到的问题：梯度消失或梯度爆炸，收敛慢，过拟合：使用dropout。

    - 注：dropout正则化：每次随机选取一些神经元不参加训练，预测时这些神经元才生效，这些神经元的输出结果要乘以一个概率值。通过这种方式，降低了神经元之间的依赖性。

  - 模型图示：

    ![image](https://github.com/SongYang6571/hello-world/blob/master/images/1.PNG)

## 循环神经网络（RNN）

- RNN概述

  - 顾名思义，是带有循环的神经网络，它可以实现将上一时刻的模型参数传入到下一时刻，从而实现模型下一时刻的输入包含上一时刻的信息。它可以理解为对模型重复赋值。

  - 模型图示：

    ![image](https://github.com/SongYang6571/hello-world/blob/master/images/2.PNG)

- **LSTM网络**

  - 长时依赖问题：当我们用传统RNN模型获取之前的输入信息时，当之前的有用信息与当前预测位置相距过远时，RNN实际时无法获取前文信息的，这个问题被称为长期依赖问题，为了解决该问题，我们使用LSTM网络。

  - LSTM与传统RNN的区别：传统RNN的神经元内部重复模块结构简单，例如只有一个tanh层：

    ![image](https://github.com/SongYang6571/hello-world/blob/master/images/3.PNG)

    而LSTM内部则是更复杂的结构，不是单一的神经网络层，而是四个神经网络层的特殊交互：

    ![image](https://github.com/SongYang6571/hello-world/blob/master/images/4.PNG)

  - LSTM的核心思想：LSTM的关键是细胞状态$Ci$，LSTM通过设计的“门”对细胞状态上的信息进行添加或删除。

  - 逐步了解LSTM

    - 遗忘门：决定从细胞状态中丢弃什么信息，该门会读取$h_{t-1}$和$x_t$输出一个0-1之间的数给所有在$c_{t-1}$中的数字。

      ![image](https://github.com/SongYang6571/hello-world/blob/master/images/5.PNG)

    - 输入门：决定更新什么值，产生状态$i_t$,一个tanh层创建新的候选值向量$\tilde{C}_t$

      ![image](https://github.com/SongYang6571/hello-world/blob/master/images/6.PNG)

    - 更新细胞状态：

      ![image](https://github.com/SongYang6571/hello-world/blob/master/images/7.PNG)

    - 输出：这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。

      ![image](https://github.com/SongYang6571/hello-world/blob/master/images/8.PNG)

- 参考：<http://colah.github.io/posts/2015-08-Understanding-LSTMs/>

## 反向传播算法（BP）

- 正向传播算法：从输入向量开始，每一隐藏层的输出为：上一层的输出*权值矩阵+偏置项，通过这种方式，最终得到最后一层的输出。
- 反向传播的计算：
  - 损失函数定义为：$L(y,\hat y)$,其中y是真实样本。为了进行梯度下降的方法学习神经网络的参数，需计算损失函数对各层参数（权重w和偏置项b）的偏导数。
  - 需求：$\frac{\partial L(y,\hat y)}{\partial W^k}$和$\frac{\partial L(y,\hat y)}{\partial b^k}$

​              根据链式求导法则：![](https://github.com/SongYang6571/hello-world/blob/master/images\9.PNG)

​             因此我们需要分别计算$\frac{\partial L(y,\hat y)}{\partial z^k} \frac{\partial z^k}{\partial w^k} \frac{\partial z^k}{\partial b^k}$

​            1)求偏置b：![](https://github.com/SongYang6571/hello-world/blob/master/images\10.PNG)

​                                   最后求出结果是一个对应每一层输出结点数的单位矩阵。

​            2)求误差项：$\frac{\partial L(y,\hat y)}{\partial z^k}$,误差项通常用$\delta^k$表示第k层神经元的误差项，其值的大小代表了第k层神经元对总          		误差的影响大小。

​		由前向计算，我们得知第k+1层输入与第k层输出的关系为：

​		![](https://github.com/SongYang6571/hello-world/blob/master/images\11.PNG)

​		又因为：$n^k=f_k(z^k)$,根据链式法则，我们可以得到$\delta^k$为：

​		

![](F:\https://github.com/SongYang6571/hello-world/blob/master/images\12.PNG)

​		由上式我们可以得出结论，第k层的误差项$\delta^k$是由k+1层误差项乘以k+1层权重，再乘以k层激活函		数的导数（梯度）得到的，这就是误差的反向传播。

​		最后我们得到用误差项表示偏导数的形式：

​		![](https://github.com/SongYang6571/hello-world/blob/master/images\13.PNG)

​               参考网址：<http://www.tensorflownews.com/2018/03/20/backpropagation-algorithm/>

